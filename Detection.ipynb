{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense,Conv2D,Flatten,Dropout,MaxPooling2D,Activation,BatchNormalization\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "#Matplotlib relies on the Pillow library to load image data.\n",
    "\n",
    "# import the opencv library\n",
    "import cv2\n",
    "# from cvzone.FaceDetectionModule import FaceDetector\n",
    "import cvzone\n",
    "# At the core cvzone uses OpenCV and Mediapipe libraries.\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimension = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load age model\n",
    "model2 = keras.models.load_model('AgeModel/age128-model-test-4.h5')\n",
    "# model2 = keras.models.load_model('AgeModel/untitled_age_61.h5')\n",
    "# model2 = keras.models.load_model('AgeModel/age64_acc_74_val_acc_63.h5')\n",
    "# model2 = keras.models.load_model('AgeModel/age128_acc_75_val_acc_62.h5')\n",
    "\n",
    "# model2 = keras.models.load_model('AgeModel/age128-model-test-2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Gender model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = keras.models.load_model('GenderModel/gender_sgd_val_89_ac_96.h5')\n",
    "# model1 = keras.models.load_model('GenderModel/gender64_sgd_val_9152_ac_9964.h5')\n",
    "model1 = keras.models.load_model('GenderModel/128gender-model-2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capturing video image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# camera = cv2.VideoCapture(0)\n",
    "# for i in range(1):\n",
    "#     return_value, img = camera.read()\n",
    "#     cv2.imwrite('opencv'+str(i)+'.jpg', img)\n",
    "# camera.release()\n",
    "\n",
    "# plt.imshow(img)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "image loading and checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_image = cv2.imread('babinn2.jpg')\n",
    "# plt.imshow(sample_image)\n",
    "# sample_image = cv2.cvtColor(sample_image,cv2.COLOR_BGR2GRAY)\n",
    "# faces = cv2.CascadeClassifier('haarcascade_frontalface_default.xml').detectMultiScale(sample_image,1.2,4)\n",
    "\n",
    "# for (x,y,w,h) in faces:\n",
    "#     cv2.rectangle(sample_image,(x,y),(x+w,y+h),(0,0,255),2)\n",
    "#     roi_gray = sample_image[y:y+w,x:x+h]\n",
    "#     roi_gray = cv2.resize(roi_gray,(dimension,dimension))\n",
    "#     image_pixels = img_to_array(roi_gray)\n",
    "#     image_pixels = np.expand_dims(image_pixels,axis=0)\n",
    "#     image_pixels /=255\n",
    "   \n",
    "#     predictions = model1.predict(image_pixels)\n",
    "#     print(predictions)\n",
    "#     if predictions[0][0]<=0.5:\n",
    "#         print('male')\n",
    "#     else:\n",
    "#         print('female')\n",
    "#     plt.imshow(roi_gray)\n",
    "\n",
    "#     predictions2 = model2.predict(image_pixels)\n",
    "#     predictions2 = np.argmax([predictions2])\n",
    "#     age_group = ['0-10','10-20','20-35','35-50','50-60','>60']\n",
    "    \n",
    "#     print('age-group :' + age_group[predictions2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "face detection using webcam\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "#     # Capture frame-by-frame\n",
    "#     ret, frame = video_capture.read()\n",
    "#     sample_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = faceCascade.detectMultiScale(gray,\n",
    "#                                          scaleFactor=1.1,\n",
    "#                                          minNeighbors=5,\n",
    "#                                          minSize=(64, 64),\n",
    "#                                          flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "        \n",
    "#     for (x,y,w,h) in faces:\n",
    "#         roi_gray = sample_image[y:y+w,x:x+h]\n",
    "#         roi_gray = cv2.resize(roi_gray,(64,64))\n",
    "#         image_pixels = img_to_array(roi_gray)\n",
    "#         image_pixels = np.expand_dims(image_pixels,axis=0)\n",
    "#         image_pixels /=255\n",
    "\n",
    "#         predictions = model1.predict(image_pixels) \n",
    "#         if predictions[0][0]<=0.5:\n",
    "#             g ='gender : male ' + str(predictions)\n",
    "#         else:\n",
    "#             g = 'gender : female ' + str(predictions)\n",
    "\n",
    "\n",
    "#         # predictions2 = model2.predict(image_pixels)\n",
    "#         # predictions2 = np.argmax([predictions2])\n",
    "#         # age_group = ['0-10','10-20','20-30','30-40','40-50','50-60','60-70','70-80','80-90','90-100','>110']\n",
    "#         # a = 'age-group : '+ str(age_group[predictions2])\n",
    "        \n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h),(0,255,0), 2)\n",
    "        \n",
    "#         #Syntax: cv2.putText(image, text, org, font, fontScale, color[, thickness[, lineType[, bottomLeftOrigin]]])\n",
    "#         cv2.putText(frame, g, (x, y-30), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "#         # cv2.putText(frame, a, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "    \n",
    "#         # Display the resulting frame\n",
    "#     cv2.imshow('Video', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# oldcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# faceCascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "# video_capture = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "#     # Capture frame-by-frame\n",
    "#     ret, frame = video_capture.read()\n",
    "#     grey_image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = faceCascade.detectMultiScale(grey_image,\n",
    "#                                          scaleFactor=1.1,\n",
    "#                                          minNeighbors=5,\n",
    "#                                          minSize=(60, 60),\n",
    "#                                          flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "#     for (x,y,w,h) in faces:\n",
    "#         roi_gray = grey_image[y:y+w,x:x+h]\n",
    "#         roi_gray = cv2.resize(roi_gray,(64,64))\n",
    "#         image_pixels = img_to_array(roi_gray)\n",
    "#         image_pixels = np.expand_dims(image_pixels,axis=0)\n",
    "#         image_pixels /=255\n",
    "#         cv2.rectangle(frame, (x, y), (x + w, y + h),(0,255,0), 2)\n",
    "\n",
    "#         predictions = model1.predict(image_pixels)\n",
    "#         if predictions[0][0]<=0.5:\n",
    "#             g ='gender : male ' + str(predictions)\n",
    "#         else:\n",
    "#             g = 'gender : female ' + str(predictions)\n",
    "\n",
    "        \n",
    "#         predictions2 = model2.predict(image_pixels)\n",
    "#         predictions2 = np.argmax([predictions2])\n",
    "#         age_group = ['0-12','12-24','24-36','36-48','48-60','>60']\n",
    "#         a = 'age-group : '+ str(age_group[predictions2])\n",
    "#         cv2.putText(frame, a, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)    \n",
    "#         cv2.putText(frame, g, (x, y-30), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "    \n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('Video', frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "# video_capture.release()\n",
    "# cv2.destroyAllWindows()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_capture = cv2.VideoCapture(0)\n",
    "mpFaceDetection = mp.solutions.face_detection\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "faceDetection = mpFaceDetection.FaceDetection()\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    success, img = video_capture.read()\n",
    "    imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    results = faceDetection.process(imgRGB)\n",
    "    \n",
    "    if results.detections:\n",
    "        for id,detection in enumerate(results.detections):\n",
    "            # print(id, detection)\n",
    "            # print(detection.score)\n",
    "            # print(detection.location_data.relative_bounding_box)\n",
    "            \n",
    "            #readymade bounding box\n",
    "            # mpDraw.draw_detection(img, detection)\n",
    "            bboxC = detection.location_data.relative_bounding_box\n",
    "            ih, iw, ic = img.shape\n",
    "            bbox = int(bboxC.xmin * iw),int(bboxC.ymin * ih), \\\n",
    "                int(bboxC.width * iw),int(bboxC.height * ih)\n",
    "            cv2.rectangle(img, bbox, (255,0,255),2)\n",
    "            \n",
    "            \n",
    "            #Image checking age and gender\n",
    "            grey_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "            roi_gray = cv2.resize(grey_image,(dimension,dimension))\n",
    "            image_pixels = img_to_array(roi_gray)\n",
    "            image_pixels = np.expand_dims(image_pixels,axis=0)\n",
    "            image_pixels /=255\n",
    "            \n",
    "            predictions = model1.predict(image_pixels)\n",
    "            if predictions[0][0]<0.5:\n",
    "                g ='gender : male ' + str(predictions)\n",
    "            else:\n",
    "                g = 'gender : female ' + str(predictions)\n",
    "            \n",
    "            predictions2 = model2.predict(image_pixels)\n",
    "            predictions2 = np.argmax([predictions2])\n",
    "            age_group = ['0-10','10-20','20-35','35-50','50-60','>60']\n",
    "            a = 'age-group : '+ str(age_group[predictions2])\n",
    "\n",
    "            cv2.putText(img, g, (bbox[0], bbox[1]-30), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "            cv2.putText(img, a, (bbox[0], bbox[1]-10), cv2.FONT_HERSHEY_SIMPLEX,0.75, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Image\", img)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
